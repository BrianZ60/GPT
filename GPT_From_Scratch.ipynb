{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+WYXJ+VGnZc2vm0fvpOw6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrianZ60/GPT/blob/main/GPT_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "_Z5IpOI15qol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d81f6a94-4add-4aa7-8136-a2f87b678c3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-08 05:57:29--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "input.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-08-08 05:57:29 (18.4 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt # web get\n",
        "\n",
        "with open(\"input.txt\", 'r', encoding=\"utf-8\") as f:\n",
        "  text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Length of data set (characters): {len(text)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNVf8aG67Rt8",
        "outputId": "15423365-213e-4809-d8f2-18469d57ca1d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of data set (characters): 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "id": "z4eaR4_P7dX5",
        "outputId": "6774ee82-33b8-40ca-fd74-91ef2dbfa4ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print(\"\".join(chars))\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zACXJKjTyMSS",
        "outputId": "2f38fef1-86f4-4c16-880d-37fca194aa4a"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer\n",
        "# our tokens will be characters\n",
        "ctoi = {ch:i for i,ch in enumerate(chars)} # dictionary of char to int\n",
        "itoc = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "# def encode(s: str): # takes in a string, outputs a list of ints\n",
        "#   return [ctoi[c] for c in s]\n",
        "\n",
        "# def decode(l): # takes in a list of ints, outputs a string\n",
        "#   return ''.join(itoc[i] for i in l)\n",
        "\n",
        "encode = lambda s: [ctoi[c] for c in s]\n",
        "decode = lambda l: ''.join(itoc[i] for i in l)"
      ],
      "metadata": {
        "id": "_Xa-7q72zYqn"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encode(\"hello world\"))\n",
        "print(decode(encode(\"hello world\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vv_BQgEE0jMZ",
        "outputId": "79dfcff8-b792-43fb-dd32-3d96022f3e08"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n",
            "hello world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize the entire data:\n",
        "# dataset\n",
        "import torch\n",
        "data = torch.tensor(encode(text))\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUn0jiymBDZ0",
        "outputId": "64c60705-4cc8-4c54-8fe5-2f0df34861f2"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split = int(0.9*len(data))\n",
        "\n",
        "train_data = data[:split]\n",
        "val_data = data[split:]"
      ],
      "metadata": {
        "id": "LxBjc7G0B6tA"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BLOCK_SIZE = 8 # maximum context length\n",
        "train_data[:BLOCK_SIZE+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-Chj68nIFBZ",
        "outputId": "86a97289-7bf2-4e04-d435-1a3a1b5ba64c"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:BLOCK_SIZE]\n",
        "y = train_data[1:BLOCK_SIZE+1]\n",
        "for t in range(BLOCK_SIZE):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"when input is {context} the target: {target}\")\n",
        "\n",
        "# the size of the input goes from 1 to the block size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGEPJWNbKlXl",
        "outputId": "8c4d031f-714f-48cf-fb7d-b840945c5899"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "BATCH_SIZE = 4\n",
        "BLOCK_SIZE = 8\n",
        "\n",
        "# dataloader\n",
        "def get_batch(split_type):\n",
        "  data = train_data if split_type == \"train\" else val_data\n",
        "  # generate batch of data of inputs x and targets y\n",
        "  offsets = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE,))\n",
        "  x = torch.stack([data[offset:offset+BLOCK_SIZE] for offset in offsets])\n",
        "  y = torch.stack([data[offset+1:offset+BLOCK_SIZE+1] for offset in offsets])\n",
        "\n",
        "  return x, y\n",
        "\n",
        "batch_x, batch_y = get_batch(\"train\")\n",
        "print(\"inputs: \")\n",
        "print(batch_x)\n",
        "\n",
        "print(\"targets: \")\n",
        "print(batch_y)\n",
        "\n",
        "print(\"\\n32 training examples:\")\n",
        "\n",
        "for b in range(BATCH_SIZE): # batch dimension\n",
        "  print(f\"batch #{b+1}\")\n",
        "  for t in range(BLOCK_SIZE): # time dimension\n",
        "    context = batch_x[b, :t+1]\n",
        "    target = batch_y[b,t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYgK6tfqLHAY",
        "outputId": "797f0db2-8d64-48ad-96d1-2e77ddcbdb54"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs: \n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets: \n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "\n",
            "32 training examples:\n",
            "batch #1\n",
            "when input is tensor([24]) the target: 43\n",
            "when input is tensor([24, 43]) the target: 58\n",
            "when input is tensor([24, 43, 58]) the target: 5\n",
            "when input is tensor([24, 43, 58,  5]) the target: 57\n",
            "when input is tensor([24, 43, 58,  5, 57]) the target: 1\n",
            "when input is tensor([24, 43, 58,  5, 57,  1]) the target: 46\n",
            "when input is tensor([24, 43, 58,  5, 57,  1, 46]) the target: 43\n",
            "when input is tensor([24, 43, 58,  5, 57,  1, 46, 43]) the target: 39\n",
            "batch #2\n",
            "when input is tensor([44]) the target: 53\n",
            "when input is tensor([44, 53]) the target: 56\n",
            "when input is tensor([44, 53, 56]) the target: 1\n",
            "when input is tensor([44, 53, 56,  1]) the target: 58\n",
            "when input is tensor([44, 53, 56,  1, 58]) the target: 46\n",
            "when input is tensor([44, 53, 56,  1, 58, 46]) the target: 39\n",
            "when input is tensor([44, 53, 56,  1, 58, 46, 39]) the target: 58\n",
            "when input is tensor([44, 53, 56,  1, 58, 46, 39, 58]) the target: 1\n",
            "batch #3\n",
            "when input is tensor([52]) the target: 58\n",
            "when input is tensor([52, 58]) the target: 1\n",
            "when input is tensor([52, 58,  1]) the target: 58\n",
            "when input is tensor([52, 58,  1, 58]) the target: 46\n",
            "when input is tensor([52, 58,  1, 58, 46]) the target: 39\n",
            "when input is tensor([52, 58,  1, 58, 46, 39]) the target: 58\n",
            "when input is tensor([52, 58,  1, 58, 46, 39, 58]) the target: 1\n",
            "when input is tensor([52, 58,  1, 58, 46, 39, 58,  1]) the target: 46\n",
            "batch #4\n",
            "when input is tensor([25]) the target: 17\n",
            "when input is tensor([25, 17]) the target: 27\n",
            "when input is tensor([25, 17, 27]) the target: 10\n",
            "when input is tensor([25, 17, 27, 10]) the target: 0\n",
            "when input is tensor([25, 17, 27, 10,  0]) the target: 21\n",
            "when input is tensor([25, 17, 27, 10,  0, 21]) the target: 1\n",
            "when input is tensor([25, 17, 27, 10,  0, 21,  1]) the target: 54\n",
            "when input is tensor([25, 17, 27, 10,  0, 21,  1, 54]) the target: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# bigram model only looks at prev token\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    # lookup table where each row corresponds to each token\n",
        "    # and the row values are the logits for the next token\n",
        "    self.embedding_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=vocab_size)\n",
        "\n",
        "  def forward(self, x, targets=None):\n",
        "    # x: (B, T)\n",
        "    logits = self.embedding_table(x) # (B, T, C)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss_fn = nn.CrossEntropyLoss()\n",
        "      loss = loss_fn(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, _ = self(idx) # (B, T, C)\n",
        "      logits = logits[:, -1, :] # (B, C) we only want the token probabilities for the last token\n",
        "      probs = torch.softmax(logits, dim=1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) Draws one sample using probabilities of probs\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "LkrtpX5vOj1X"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test generate:"
      ],
      "metadata": {
        "id": "e4DlLDtArUWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "\n",
        "new_idx = model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)\n",
        "print(decode(new_idx[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQKciXJ3rTFJ",
        "outputId": "376eeb2d-47d9-4ace-b961-2a6e793657b2"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.001) # adam with weight decay to prevent overfitting"
      ],
      "metadata": {
        "id": "v8xRyA80uhYP"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "for steps in range(100):\n",
        "  x_batch, y_batch = get_batch(\"train\")\n",
        "\n",
        "  logits, loss = model(x_batch, y_batch)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWDnWtVR1uA7",
        "outputId": "07dcb5d8-37cc-4db3-af56-954bf6099ada"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.587916374206543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcYqZnDq4idf",
        "outputId": "83aa6f43-d79f-4f62-f701-80468b48aa87"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "xiKi-RJ:CgqVuUa!U?qMH.uk!sCuMXvv!CJFfx;LgRyJknOEti.?I&-gPlLyulId?XlaInQ'q,lT$\n",
            "3Q&sGlvHQ?mqSq-eON\n",
            "x?SP fUAfCAuCX:bOlgiRQWN:Mphaw\n",
            "tRLKuYXEaAXxrcq-gCUzeh3w!AcyaylgYWjmJM?Uzw:inaY,:C&OECW:vmGGJAn3onAuMgia!ms$Vb q-gCOcPcUhOnxJGUGSPJWT:.?ujmJFoiNL&A'DxY,prZ?qdT;hoo'dHooXXlxf'WkHK&u3Q?rqUi.kz;?Yx?C&u3Qbfzxlyh'Vl:zyxjKXgC?\n",
            "lv'QKFiBeviNxO'm!Upm$srm&TqViqiBD3HBP!juEOpmZJyF$Fwfy!PlvWPFC\n",
            "&WDdP!Ko,px\n",
            "x\n",
            "tREOE;AJ.BeXkylOVD3KHp$e?nD,.SFbWWI'ubcL!q-tU;aXmJ&uGXHxJXI&Z!gHRpajj;l.\n",
            "pTErIBjx;JKIgoCnLGXrJSP!AU-AcbczR?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mathematical trick in self-attention"
      ],
      "metadata": {
        "id": "UN4t4i35Mgyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# toy example:\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2\n",
        "x=torch.randn(B,T,C)\n",
        "\n",
        "# tokens need to communicate but can only communicate with past tokens"
      ],
      "metadata": {
        "id": "OGEwSjpFMkQT"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# easiest way to do this is by averaging all previous contexts (weighted aggregation)\n",
        "\n",
        "# version 1\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    x_prev = x[b, :t+1] #(t+1, C)\n",
        "    xbow[b,t] = torch.mean(x_prev, dim=0) # (C,)"
      ],
      "metadata": {
        "id": "ZMuRDDPUNOL8"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gn8-84jaQAei",
        "outputId": "1e9fa5c8-3819-47a7-d672-e73029c06e23"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1808, -0.0700],\n",
              "        [-0.3596, -0.9152],\n",
              "        [ 0.6258,  0.0255],\n",
              "        [ 0.9545,  0.0643],\n",
              "        [ 0.3612,  1.1679],\n",
              "        [-1.3499, -0.5102],\n",
              "        [ 0.2360, -0.2398],\n",
              "        [-0.9211,  1.5433]])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPa1UAI4QBSo",
        "outputId": "0bd5023a-265b-4e29-91da-c08f9aacee19"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1808, -0.0700],\n",
              "        [-0.0894, -0.4926],\n",
              "        [ 0.1490, -0.3199],\n",
              "        [ 0.3504, -0.2238],\n",
              "        [ 0.3525,  0.0545],\n",
              "        [ 0.0688, -0.0396],\n",
              "        [ 0.0927, -0.0682],\n",
              "        [-0.0341,  0.1332]])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can do this much faster with matrix multi:"
      ],
      "metadata": {
        "id": "mgjPJxOKQQXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3)) # triangulates the matrix\n",
        "a = a / torch.sum(a, dim=1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eHpZsToSBKi",
        "outputId": "784b158f-fdf1-4fe1-ac87-a2d2b812b625"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torch.tril(torch.ones(T, T))\n",
        "weights /= weights.sum(1, keepdim=True)\n",
        "weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTOUPa2iSu3T",
        "outputId": "f4c77b60-18ed-4b2e-de9e-d72b630876f7"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 2\n",
        "xbow2 = weights @ x # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "xbow[0], xbow2[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ea8LYHkTT4h",
        "outputId": "f05f84b4-e82c-4ce7-8b78-a2e10b050628"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.1808, -0.0700],\n",
              "         [-0.0894, -0.4926],\n",
              "         [ 0.1490, -0.3199],\n",
              "         [ 0.3504, -0.2238],\n",
              "         [ 0.3525,  0.0545],\n",
              "         [ 0.0688, -0.0396],\n",
              "         [ 0.0927, -0.0682],\n",
              "         [-0.0341,  0.1332]]),\n",
              " tensor([[ 0.1808, -0.0700],\n",
              "         [-0.0894, -0.4926],\n",
              "         [ 0.1490, -0.3199],\n",
              "         [ 0.3504, -0.2238],\n",
              "         [ 0.3525,  0.0545],\n",
              "         [ 0.0688, -0.0396],\n",
              "         [ 0.0927, -0.0682],\n",
              "         [-0.0341,  0.1332]]))"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 3 (what we're going to use)\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T, T)) # think of this as the relationships (affinities) between every token (init to 0 ofc)\n",
        "wei = wei.masked_fill(tril == 0, float(\"-inf\")) # we prevent tokens from interacting with future tokens\n",
        "wei = torch.softmax(wei, dim=1)\n",
        "\n",
        "xbow3 = wei @ x\n",
        "xbow3[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBOkR8_OWnvs",
        "outputId": "69f25456-65aa-42b2-bb73-a35565438d9d"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1808, -0.0700],\n",
              "        [-0.0894, -0.4926],\n",
              "        [ 0.1490, -0.3199],\n",
              "        [ 0.3504, -0.2238],\n",
              "        [ 0.3525,  0.0545],\n",
              "        [ 0.0688, -0.0396],\n",
              "        [ 0.0927, -0.0682],\n",
              "        [-0.0341,  0.1332]])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Works b/c softmax is $\\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}$"
      ],
      "metadata": {
        "id": "8HecjmPIYMVx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleanup and position embeddings"
      ],
      "metadata": {
        "id": "q8uvFXaKrlP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "BATCH_SIZE = 32\n",
        "BLOCK_SIZE = 8\n",
        "MAX_ITERS = 3000\n",
        "EVAL_INTERVAL = 300\n",
        "LEARNING_RATE = 0.01\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "EVAL_ITERS = 200\n",
        "NUM_EMBD = 32 # number of embed dimensions\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt # web get\n",
        "\n",
        "with open(\"input.txt\", 'r', encoding=\"utf-8\") as f:\n",
        "  text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "ctoi = {ch:i for i,ch in enumerate(chars)} # dictionary of char to int\n",
        "itoc = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [ctoi[c] for c in s]\n",
        "decode = lambda l: ''.join(itoc[i] for i in l)\n",
        "\n",
        "data = torch.tensor(encode(text))\n",
        "split = int(0.9*len(data))\n",
        "train_data = data[:split]\n",
        "val_data = data[split:]\n",
        "\n",
        "def get_batch(split_type):\n",
        "  data = train_data if split_type == \"train\" else val_data\n",
        "  # generate batch of data of inputs x and targets y\n",
        "  offsets = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE,))\n",
        "  x = torch.stack([data[offset:offset+BLOCK_SIZE] for offset in offsets])\n",
        "  y = torch.stack([data[offset+1:offset+BLOCK_SIZE+1] for offset in offsets])\n",
        "\n",
        "  return x, y\n",
        "\n",
        "def estimate_loss():\n",
        "    out = {} # empty dict\n",
        "    model.eval()\n",
        "\n",
        "    with torch.inference_mode():\n",
        "      for split in [\"train\", \"val\"]:\n",
        "          losses = torch.zeros(EVAL_ITERS)\n",
        "          for i in range(EVAL_ITERS):\n",
        "              X, Y = get_batch(split)\n",
        "              logits, loss = model(X, Y)\n",
        "              losses[i] = loss.item()\n",
        "          out[split] = losses.mean()\n",
        "\n",
        "    return out\n",
        "\n",
        "# bigram model only looks at prev token\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # lookup table where each row corresponds to each token\n",
        "    # and the row values are the logits for the next token\n",
        "    self.token_embedding_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=NUM_EMBD)\n",
        "    self.position_embedding_table = nn.Embedding(num_embeddings=BLOCK_SIZE, embedding_dim=NUM_EMBD)\n",
        "    self.lm_head = nn.Linear(in_features=NUM_EMBD, out_features=vocab_size) # language modeling head\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # idx: (B, T)\n",
        "    B, T = idx.shape\n",
        "    token_emb = self.token_embedding_table(idx) # (B, T, C) # C here is num_embd\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
        "    x = token_emb + pos_emb # (B, T, C)\n",
        "    logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss_fn = nn.CrossEntropyLoss()\n",
        "      loss = loss_fn(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, _ = self(idx) # (B, T, C)\n",
        "      logits = logits[:, -1, :] # (B, C) we only want the token probabilities for the last token\n",
        "      probs = torch.softmax(logits, dim=1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) Draws one sample using probabilities of probs\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "for iter in range(MAX_ITERS):\n",
        "\n",
        "  x_batch, y_batch = get_batch(\"train\")\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  logits, loss = model(x_batch, y_batch)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if iter % EVAL_INTERVAL == 0:\n",
        "      losses = estimate_loss()\n",
        "      print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "# context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "# print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zbUcWy8g5b-",
        "outputId": "1b6d1bde-e892-428f-894d-f2eb6051b041"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.3176, val loss 4.3225\n",
            "step 300: train loss 2.5404, val loss 2.5585\n",
            "step 600: train loss 2.5178, val loss 2.5344\n",
            "step 900: train loss 2.4965, val loss 2.5146\n",
            "step 1200: train loss 2.5086, val loss 2.5240\n",
            "step 1500: train loss 2.4873, val loss 2.5108\n",
            "step 1800: train loss 2.4974, val loss 2.5201\n",
            "step 2100: train loss 2.4958, val loss 2.5096\n",
            "step 2400: train loss 2.4940, val loss 2.5098\n",
            "step 2700: train loss 2.5050, val loss 2.5139\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# v4: self-attention\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# a single head performing self-attention\n",
        "\n",
        "# query vector: what the token is looking for\n",
        "# key vector: what the token contains (labels)\n",
        "# wei becomes every query @ every key\n",
        "\n",
        "HEAD_SIZE = 16\n",
        "key = nn.Linear(C, HEAD_SIZE, bias=False) # key matrix\n",
        "query = nn.Linear(C, HEAD_SIZE, bias=False) # query matrix\n",
        "value = nn.Linear(C, HEAD_SIZE, bias=False) # value is the actual information tied to each key\n",
        "\n",
        "k = key(x) # (B, T, HEAD_SIZE)\n",
        "q = query(x) # (B, T, HEAD_SIZE)\n",
        "# each token now has a key and query\n",
        "\n",
        "# transpose k by swapping T and HEAD_SIZE dims\n",
        "# wei = q @ k.transpose(1, -1) # (B, T, T)\n",
        "wei = q @ k.transpose(1, -1) * HEAD_SIZE ** -0.5 # scaled attention\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "# wei = torch.zeros((T, T))\n",
        "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
        "wei = torch.softmax(wei, dim=2)\n",
        "\n",
        "v = value(x)\n",
        "\n",
        "# out = wei @ x\n",
        "out = wei @ v #  (B, T, T) @ (B, T, HEAD_SIZE) --> (B, T, HEAD_SIZE)\n",
        "\n",
        "wei[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wat1VO0vuJRT",
        "outputId": "88044644-8d3a-4d96-cb5d-d4788792a2d2"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3966, 0.6034, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3069, 0.2892, 0.4039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3233, 0.2175, 0.2443, 0.2149, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1479, 0.2034, 0.1663, 0.1455, 0.3369, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1259, 0.2490, 0.1324, 0.1062, 0.3141, 0.0724, 0.0000, 0.0000],\n",
              "        [0.1598, 0.1990, 0.1140, 0.1125, 0.1418, 0.1669, 0.1061, 0.0000],\n",
              "        [0.0845, 0.1197, 0.1078, 0.1537, 0.1086, 0.1146, 0.1558, 0.1553]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(NUM_EMBD, head_size, bias=False)\n",
        "    self.query = nn.Linear(NUM_EMBD, head_size, bias=False)\n",
        "    self.value = nn.Linear(NUM_EMBD, head_size, bias=False)\n",
        "    self.register_buffer(\"tril\", torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE))) # we don't want it to be a parameter\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "    k=self.key(x)\n",
        "    q=self.query(x)\n",
        "\n",
        "    wei = q @ k.transpose(1, -1) * C**-0.5\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\")) # slice out a TxT chunk of tril\n",
        "    wei = torch.softmax(wei, dim=-1)\n",
        "\n",
        "    v = self.value(x)\n",
        "    return wei @ v"
      ],
      "metadata": {
        "id": "oRPfVdd0D5O_"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "BATCH_SIZE = 32\n",
        "BLOCK_SIZE = 8\n",
        "MAX_ITERS = 5000\n",
        "EVAL_INTERVAL = 500\n",
        "LEARNING_RATE = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "EVAL_ITERS = 200\n",
        "NUM_EMBD = 32 # number of embed dimensions\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt # web get\n",
        "\n",
        "with open(\"input.txt\", 'r', encoding=\"utf-8\") as f:\n",
        "  text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "ctoi = {ch:i for i,ch in enumerate(chars)} # dictionary of char to int\n",
        "itoc = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [ctoi[c] for c in s]\n",
        "decode = lambda l: ''.join(itoc[i] for i in l)\n",
        "\n",
        "data = torch.tensor(encode(text))\n",
        "split = int(0.9*len(data))\n",
        "train_data = data[:split]\n",
        "val_data = data[split:]\n",
        "\n",
        "def get_batch(split_type):\n",
        "  data = train_data if split_type == \"train\" else val_data\n",
        "  # generate batch of data of inputs x and targets y\n",
        "  offsets = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE,))\n",
        "  x = torch.stack([data[offset:offset+BLOCK_SIZE] for offset in offsets])\n",
        "  y = torch.stack([data[offset+1:offset+BLOCK_SIZE+1] for offset in offsets])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "\n",
        "  return x, y\n",
        "\n",
        "def estimate_loss():\n",
        "    out = {} # empty dict\n",
        "    model.eval()\n",
        "\n",
        "    with torch.inference_mode():\n",
        "      for split in [\"train\", \"val\"]:\n",
        "          losses = torch.zeros(EVAL_ITERS)\n",
        "          for i in range(EVAL_ITERS):\n",
        "              X, Y = get_batch(split)\n",
        "              logits, loss = model(X, Y)\n",
        "              losses[i] = loss.item()\n",
        "          out[split] = losses.mean()\n",
        "\n",
        "    return out\n",
        "\n",
        "# bigram model only looks at prev token\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # lookup table where each row corresponds to each token\n",
        "    # and the row values are the logits for the next token\n",
        "    self.token_embedding_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=NUM_EMBD)\n",
        "    self.position_embedding_table = nn.Embedding(num_embeddings=BLOCK_SIZE, embedding_dim=NUM_EMBD)\n",
        "    self.sa_head = Head(NUM_EMBD)\n",
        "    self.lm_head = nn.Linear(in_features=NUM_EMBD, out_features=vocab_size) # language modeling head\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # idx: (B, T)\n",
        "    B, T = idx.shape\n",
        "    token_emb = self.token_embedding_table(idx) # (B, T, C) # C here is num_embd\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
        "    x = token_emb + pos_emb # (B, T, C)\n",
        "    x = self.sa_head(x)\n",
        "    logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss_fn = nn.CrossEntropyLoss()\n",
        "      loss = loss_fn(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_crop = idx[:, -BLOCK_SIZE:] # only get last block_size tokens that way don't go out of scope for position embedding\n",
        "      logits, _ = self(idx_crop) # (B, T, C)\n",
        "      logits = logits[:, -1, :] # (B, C) we only want the token probabilities for the last token\n",
        "      probs = torch.softmax(logits, dim=1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) Draws one sample using probabilities of probs\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "for iter in range(MAX_ITERS):\n",
        "\n",
        "  x_batch, y_batch = get_batch(\"train\")\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  logits, loss = model(x_batch, y_batch)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if iter % EVAL_INTERVAL == 0:\n",
        "      losses = estimate_loss()\n",
        "      print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56isKGTPOibN",
        "outputId": "10cdb767-80de-4094-9fd1-39e09a50e8c9"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.1854, val loss 4.1905\n",
            "step 500: train loss 2.7224, val loss 2.7363\n",
            "step 1000: train loss 2.5658, val loss 2.5730\n",
            "step 1500: train loss 2.5229, val loss 2.5194\n",
            "step 2000: train loss 2.4769, val loss 2.4866\n",
            "step 2500: train loss 2.4586, val loss 2.4678\n",
            "step 3000: train loss 2.4492, val loss 2.4562\n",
            "step 3500: train loss 2.4305, val loss 2.4546\n",
            "step 4000: train loss 2.4278, val loss 2.4358\n",
            "step 4500: train loss 2.4180, val loss 2.4321\n",
            "\n",
            "IUC-ELUIICIShaen, hwyar sanst es--\n",
            "tpthe br'he p VO:\n",
            "bay tUEVO:'dwo othaly Walod bere dherreuts.ogek,l betrath.\n",
            "\n",
            "R KOENGHACTer'dr Heer, utttho ot suins, ivyRCe es'ust\n",
            "y'Thitiy Ofaro:\n",
            "o loumy wm.\n",
            "\n",
            "Th ke ine REde er'spethe anetrses he ties ius cheh feltatous the.\n",
            "\n",
            "A\n",
            "Y  thatihneua nd thatrl cu mussee y wholuls:alde pgilwe h mis bule lr.\n",
            "Cawiene se hallyo misin casut\n",
            "er sten batretnl\n",
            "St-Nhir,ve abdeSam! ste msou tohe'f shagcw uures ds baghine. Git.\n",
            "TOas sh I'thmy, skss, h's lfant, r'ingthis nta;\n",
            "The\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Muli-Head Attention:"
      ],
      "metadata": {
        "id": "uKaMYT_0RzO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.cat([head(x) for head in self.heads], dim=-1) # concat over the C dimension"
      ],
      "metadata": {
        "id": "sfKXQaUNR1NU"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # lookup table where each row corresponds to each token\n",
        "    # and the row values are the logits for the next token\n",
        "    self.token_embedding_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=NUM_EMBD)\n",
        "    self.position_embedding_table = nn.Embedding(num_embeddings=BLOCK_SIZE, embedding_dim=NUM_EMBD)\n",
        "    self.sa_heads = MultiHeadAttention(4, NUM_EMBD//4) # split num_embd over num_heads\n",
        "    self.lm_head = nn.Linear(in_features=NUM_EMBD, out_features=vocab_size) # language modeling head\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # idx: (B, T)\n",
        "    B, T = idx.shape\n",
        "    token_emb = self.token_embedding_table(idx) # (B, T, C) # C here is num_embd\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
        "    x = token_emb + pos_emb # (B, T, C)\n",
        "    x = self.sa_heads(x)\n",
        "    logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss_fn = nn.CrossEntropyLoss()\n",
        "      loss = loss_fn(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_crop = idx[:, -BLOCK_SIZE:] # only get last block_size tokens that way don't go out of scope for position embedding\n",
        "      logits, _ = self(idx_crop) # (B, T, C)\n",
        "      logits = logits[:, -1, :] # (B, C) we only want the token probabilities for the last token\n",
        "      probs = torch.softmax(logits, dim=1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) Draws one sample using probabilities of probs\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "DaItQakwTCvK"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel()\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "for iter in range(MAX_ITERS):\n",
        "\n",
        "  x_batch, y_batch = get_batch(\"train\")\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  logits, loss = model(x_batch, y_batch)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if iter % EVAL_INTERVAL == 0:\n",
        "      losses = estimate_loss()\n",
        "      print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "md3ckDxcTtiH",
        "outputId": "bcb5abec-d5fa-4f51-9f42-b1d1003c6234"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.2276, val loss 4.2346\n",
            "step 500: train loss 2.6234, val loss 2.6257\n",
            "step 1000: train loss 2.4615, val loss 2.4663\n",
            "step 1500: train loss 2.3995, val loss 2.4074\n",
            "step 2000: train loss 2.3611, val loss 2.3825\n",
            "step 2500: train loss 2.3289, val loss 2.3602\n",
            "step 3000: train loss 2.3122, val loss 2.3338\n",
            "step 3500: train loss 2.2996, val loss 2.3111\n",
            "step 4000: train loss 2.2824, val loss 2.3140\n",
            "step 4500: train loss 2.2576, val loss 2.2852\n",
            "\n",
            "I wroumst.\n",
            "KLO:\n",
            "GAUSondt--'l, can, EA:\n",
            "MUMET:\n",
            "Cof cing.\n",
            "\n",
            "\n",
            "O cows blin mess cise moks lis noonowe mand pemnenkar Coond eat.\n",
            "\n",
            "'s-\n",
            "Or:\n",
            "Thim of, rimenng ry. fow.\n",
            "Thrithie Vodin deoor, veout thyoulds in risteng aive youn be plavickas tampe he hes tot ay sllir I'CAnoulst mad it-zis ant's ant samel velin:\n",
            "The mo amy I tooritheth and furse oupporknot have ow sith thoull me id, my goou and be\n",
            "As sest gay, Nyou thev:\n",
            "Wobe cawte ind's furend; dy,\n",
            "Shild And\n",
            "Noldeanty solm. 'd Hoouen Lye.\n",
            "\n",
            "Bun you songcouy f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the FFN:"
      ],
      "metadata": {
        "id": "jHa4vnGLUylc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  # allows the tokens to think on all the data they gathered from sa individually\n",
        "  def __init__(self, num_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(num_embd, num_embd),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "-jJBFrC8U4Iw"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # lookup table where each row corresponds to each token\n",
        "    # and the row values are the logits for the next token\n",
        "    self.token_embedding_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=NUM_EMBD)\n",
        "    self.position_embedding_table = nn.Embedding(num_embeddings=BLOCK_SIZE, embedding_dim=NUM_EMBD)\n",
        "    self.sa_heads = MultiHeadAttention(4, NUM_EMBD//4) # split num_embd over num_heads\n",
        "    self.ffn = FeedForward(NUM_EMBD)\n",
        "    self.lm_head = nn.Linear(in_features=NUM_EMBD, out_features=vocab_size) # language modeling head\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # idx: (B, T)\n",
        "    B, T = idx.shape\n",
        "    token_emb = self.token_embedding_table(idx) # (B, T, C) # C here is num_embd\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
        "    x = token_emb + pos_emb # (B, T, C)\n",
        "    x = self.sa_heads(x)\n",
        "    x = self.ffn(x)\n",
        "    logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss_fn = nn.CrossEntropyLoss()\n",
        "      loss = loss_fn(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_crop = idx[:, -BLOCK_SIZE:] # only get last block_size tokens that way don't go out of scope for position embedding\n",
        "      logits, _ = self(idx_crop) # (B, T, C)\n",
        "      logits = logits[:, -1, :] # (B, C) we only want the token probabilities for the last token\n",
        "      probs = torch.softmax(logits, dim=1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) Draws one sample using probabilities of probs\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "YtK-8mM_VvAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer Blocks"
      ],
      "metadata": {
        "id": "tWeyUxQ2WQmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffn = FeedForward(n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.ffn(self.sa(x))"
      ],
      "metadata": {
        "id": "AR677pPGWyGc"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # lookup table where each row corresponds to each token\n",
        "    # and the row values are the logits for the next token\n",
        "    self.token_embedding_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=NUM_EMBD)\n",
        "    self.position_embedding_table = nn.Embedding(num_embeddings=BLOCK_SIZE, embedding_dim=NUM_EMBD)\n",
        "    self.blocks = nn.Sequential(\n",
        "        Block(n_embd=NUM_EMBD, n_head=4),\n",
        "        Block(n_embd=NUM_EMBD, n_head=4),\n",
        "        Block(n_embd=NUM_EMBD, n_head=4),\n",
        "    )\n",
        "    self.lm_head = nn.Linear(in_features=NUM_EMBD, out_features=vocab_size) # language modeling head\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # idx: (B, T)\n",
        "    B, T = idx.shape\n",
        "    token_emb = self.token_embedding_table(idx) # (B, T, C) # C here is num_embd\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
        "    x = token_emb + pos_emb # (B, T, C)\n",
        "    x = self.blocks(x)\n",
        "    logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss_fn = nn.CrossEntropyLoss()\n",
        "      loss = loss_fn(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_crop = idx[:, -BLOCK_SIZE:] # only get last block_size tokens that way don't go out of scope for position embedding\n",
        "      logits, _ = self(idx_crop) # (B, T, C)\n",
        "      logits = logits[:, -1, :] # (B, C) we only want the token probabilities for the last token\n",
        "      probs = torch.softmax(logits, dim=1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) Draws one sample using probabilities of probs\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "RyiDGLvMXbCK"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Residual Connections\n",
        "TODO: explain it well pls"
      ],
      "metadata": {
        "id": "4swkBIscYkO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffn = FeedForward(n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(x)\n",
        "    x = x + self.ffn(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "ygywRV-qYs7O"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also need the projection layer in multi-head & ffn"
      ],
      "metadata": {
        "id": "pAVkqQN4Y7hI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(NUM_EMBD, NUM_EMBD) # output projection\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([head(x) for head in self.heads], dim=-1) # concat over the C dimension\n",
        "    out = self.proj(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "VCRj3yFsY-fX"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  # allows the tokens to think on all the data they gathered from sa individually\n",
        "  def __init__(self, num_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(num_embd, 4*num_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*num_embd, num_embd)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "aKIuAsEzZt-o"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel()\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "for iter in range(MAX_ITERS):\n",
        "\n",
        "  x_batch, y_batch = get_batch(\"train\")\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  logits, loss = model(x_batch, y_batch)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if iter % EVAL_INTERVAL == 0 or iter == MAX_ITERS - 1::\n",
        "      losses = estimate_loss()\n",
        "      print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImVubSd0aLs2",
        "outputId": "79979f68-5da2-4cf5-d9a0-9d444c50f58f"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.4411, val loss 4.4346\n",
            "step 500: train loss 2.4106, val loss 2.4146\n",
            "step 1000: train loss 2.2832, val loss 2.2907\n",
            "step 1500: train loss 2.2114, val loss 2.2307\n",
            "step 2000: train loss 2.1528, val loss 2.1880\n",
            "step 2500: train loss 2.1156, val loss 2.1749\n",
            "step 3000: train loss 2.0745, val loss 2.1333\n",
            "step 3500: train loss 2.0570, val loss 2.1348\n",
            "step 4000: train loss 2.0295, val loss 2.0918\n",
            "step 4500: train loss 2.0189, val loss 2.0986\n",
            "\n",
            "Wast;\n",
            "Yorch\n",
            "wer driangmore, for hall pere towslerenpery her.\n",
            "The stand, I shoriurunedmain,\n",
            "It uspikes\n",
            "dand plity, spent that' pratith?\n",
            "\n",
            "LUCHYVOLUED:\n",
            "That unatt:\n",
            "Cladnapen.\n",
            "Ke forsiur,\n",
            "Andso say your lume un traptaintorman reses as he tight\n",
            "And will no-bereasellught sondst he vant'll and's thee rigalether- to dAk to cleangentlesigre.\n",
            "Whonesser:\n",
            "Hatughing brecinrefore when shonst.\n",
            "To\n",
            "Farl Lordeler.\n",
            "\n",
            "CAmmon melo\n",
            "mee wered as,\n",
            "You a grooin.\n",
            "\n",
            "BROMBEONE:\n",
            "Bonid:\n",
            "I wathost\n",
            "Thanst I\n",
            "To mulaw torsious ang\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layer normalization\n",
        "\n",
        "come back to write implementation"
      ],
      "metadata": {
        "id": "OFJ6s05GceJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffn = FeedForward(n_embd)\n",
        "    self.layer_norm_1 = nn.LayerNorm(n_embd)\n",
        "    self.layer_norm_2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(self.layer_norm_1(x))\n",
        "    x = x + self.ffn(self.layer_norm_2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "8NlNPRBBciyh"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # lookup table where each row corresponds to each token\n",
        "    # and the row values are the logits for the next token\n",
        "    self.token_embedding_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=NUM_EMBD)\n",
        "    self.position_embedding_table = nn.Embedding(num_embeddings=BLOCK_SIZE, embedding_dim=NUM_EMBD)\n",
        "    self.blocks = nn.Sequential(\n",
        "        Block(n_embd=NUM_EMBD, n_head=4),\n",
        "        Block(n_embd=NUM_EMBD, n_head=4),\n",
        "        Block(n_embd=NUM_EMBD, n_head=4),\n",
        "        nn.LayerNorm(NUM_EMBD)\n",
        "    )\n",
        "    self.lm_head = nn.Linear(in_features=NUM_EMBD, out_features=vocab_size) # language modeling head\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # idx: (B, T)\n",
        "    B, T = idx.shape\n",
        "    token_emb = self.token_embedding_table(idx) # (B, T, C) # C here is num_embd\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
        "    x = token_emb + pos_emb # (B, T, C)\n",
        "    x = self.blocks(x)\n",
        "    logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss_fn = nn.CrossEntropyLoss()\n",
        "      loss = loss_fn(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_crop = idx[:, -BLOCK_SIZE:] # only get last block_size tokens that way don't go out of scope for position embedding\n",
        "      logits, _ = self(idx_crop) # (B, T, C)\n",
        "      logits = logits[:, -1, :] # (B, C) we only want the token probabilities for the last token\n",
        "      probs = torch.softmax(logits, dim=1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) Draws one sample using probabilities of probs\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "8SbmInQ4diHT"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling it up"
      ],
      "metadata": {
        "id": "H3-Qn3AkeHe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_HEADS = 6\n",
        "NUM_LAYERS = 6\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # lookup table where each row corresponds to each token\n",
        "    # and the row values are the logits for the next token\n",
        "    self.token_embedding_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=NUM_EMBD)\n",
        "    self.position_embedding_table = nn.Embedding(num_embeddings=BLOCK_SIZE, embedding_dim=NUM_EMBD)\n",
        "    self.blocks = nn.Sequential(*[Block(n_embd=NUM_EMBD, n_head=NUM_HEADS) for _ in range(NUM_LAYERS)]) # * just unpacks the list\n",
        "    self.layer_norm = nn.LayerNorm(NUM_EMBD)\n",
        "    self.lm_head = nn.Linear(in_features=NUM_EMBD, out_features=vocab_size) # language modeling head\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # idx: (B, T)\n",
        "    B, T = idx.shape\n",
        "    token_emb = self.token_embedding_table(idx) # (B, T, C) # C here is num_embd\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
        "    x = token_emb + pos_emb # (B, T, C)\n",
        "    x = self.blocks(x)\n",
        "    x = self.layer_norm(x)\n",
        "    logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss_fn = nn.CrossEntropyLoss()\n",
        "      loss = loss_fn(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_crop = idx[:, -BLOCK_SIZE:] # only get last block_size tokens that way don't go out of scope for position embedding\n",
        "      logits, _ = self(idx_crop) # (B, T, C)\n",
        "      logits = logits[:, -1, :] # (B, C) we only want the token probabilities for the last token\n",
        "      probs = torch.softmax(logits, dim=1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) Draws one sample using probabilities of probs\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "343k4o_YeIKo"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropouts"
      ],
      "metadata": {
        "id": "ZF5ym3cifVj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DROPOUT_PROB = 0.2\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  # allows the tokens to think on all the data they gathered from sa individually\n",
        "  def __init__(self, num_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(num_embd, 4*num_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*num_embd, num_embd),\n",
        "        nn.Dropout(DROPOUT_PROB)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "B3-kfpNBfWaB"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(NUM_EMBD, NUM_EMBD) # output projection\n",
        "    self.dropout = nn.Dropout(DROPOUT_PROB)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([head(x) for head in self.heads], dim=-1) # concat over the C dimension\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out"
      ],
      "metadata": {
        "id": "CLIszqGZfq1o"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(NUM_EMBD, head_size, bias=False)\n",
        "    self.query = nn.Linear(NUM_EMBD, head_size, bias=False)\n",
        "    self.value = nn.Linear(NUM_EMBD, head_size, bias=False)\n",
        "    self.register_buffer(\"tril\", torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE))) # we don't want it to be a parameter\n",
        "\n",
        "    self.dropout = nn.Dropout(DROPOUT_PROB)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "    k=self.key(x)\n",
        "    q=self.query(x)\n",
        "\n",
        "    wei = q @ k.transpose(1, -1) * C**-0.5\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\")) # slice out a TxT chunk of tril\n",
        "    wei = torch.softmax(wei, dim=-1)\n",
        "\n",
        "    wei = self.dropout(wei)\n",
        "\n",
        "    v = self.value(x)\n",
        "    return wei @ v"
      ],
      "metadata": {
        "id": "BCxb3jjzgCXh"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# hyperparameters\n",
        "BATCH_SIZE = 64\n",
        "BLOCK_SIZE = 64\n",
        "MAX_ITERS = 5000\n",
        "EVAL_INTERVAL = 500\n",
        "LEARNING_RATE = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "EVAL_ITERS = 200\n",
        "NUM_EMBD = 96 # number of embed dimensions\n",
        "NUM_HEADS = 6\n",
        "NUM_LAYERS = 4\n",
        "DROPOUT_PROB = 0.2\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "for iter in tqdm(range(MAX_ITERS)):\n",
        "\n",
        "  x_batch, y_batch = get_batch(\"train\")\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  logits, loss = model(x_batch, y_batch)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if iter % EVAL_INTERVAL == 0 or iter == MAX_ITERS - 1:\n",
        "      losses = estimate_loss()\n",
        "      print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "IeOFvBObgvbE",
        "outputId": "702e7348-6392-4201-b875-14c8a8802416"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/5000 [01:04<89:31:12, 64.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.2600, val loss 4.2527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 309/5000 [03:44<56:47,  1.38it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-571359778.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m   \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3234198451.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mpos_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (T, C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_emb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpos_emb\u001b[0m \u001b[0;31m# (B, T, C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, T, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-133611239.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3651197397.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# concat over the C dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3651197397.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# concat over the C dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4123686393.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mwei\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mwei\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwei\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtril\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# slice out a TxT chunk of tril\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mwei\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwei\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1913\u001b[0m     \u001b[0;31m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m     \u001b[0;31m# https://github.com/pytorch/pytorch/pull/115074\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1915\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Module\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_parameters\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_parameters\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}